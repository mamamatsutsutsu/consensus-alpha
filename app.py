import streamlit as st
import pandas as pd
import plotly.graph_objects as go
from google import genai
import requests
import json
from io import StringIO
from datetime import datetime, timezone
import time
import xml.etree.ElementTree as ET

# --- 1. è¨­å®š & UIæ§‹æˆ ---
st.set_page_config(page_title="ConsensusAlpha Global v2", layout="wide")
st.title("ğŸ§  ConsensusAlpha: ã‚°ãƒ­ãƒ¼ãƒãƒ«æŠ•è³‡å§”å“¡ä¼š v2.0")

# --- 2. ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ & APIè¨­å®š ---
# Secretsã‹ã‚‰å–å¾—ã€ãªã‘ã‚Œã°ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼‰
api_key = st.secrets.get("GEMINI_API_KEY")
if not api_key:
    api_key = st.sidebar.text_input("Gemini API Key (Local only)", type="password")

if not api_key:
    st.warning("APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚Streamlitã®Secretsã«è¨­å®šã™ã‚‹ã‹ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.stop()

client = genai.Client(api_key=api_key)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
st.sidebar.header("ğŸ”§ åˆ†æè¨­å®š")
market = st.sidebar.selectbox("åˆ†æå¯¾è±¡ã®å¸‚å ´", ["ç±³å›½æ ª (US)", "æ—¥æœ¬æ ª (JP)"])

if market == "ç±³å›½æ ª (US)":
    DEFAULT_TICKERS = ["NVDA", "AAPL", "TSLA", "MSFT", "GOOGL"]
    market_suffix = "US"
    news_params = {"hl": "en-US", "gl": "US", "ceid": "US:en"}
else:
    DEFAULT_TICKERS = ["7203", "6758", "9984", "8035", "6857"]
    market_suffix = "JP"
    news_params = {"hl": "ja-JP", "gl": "JP", "ceid": "JP:ja"}

tickers_input = st.sidebar.text_input("éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰", value=",".join(DEFAULT_TICKERS))
TICKERS = [t.strip() for t in tickers_input.split(",")]
RISK_DD_REJECT = -40.0

# --- 3. å …ç‰¢ãªãƒ‡ãƒ¼ã‚¿å–å¾—ã‚¨ãƒ³ã‚¸ãƒ³ ---

def fetch_news_headlines(ticker, params):
    """å¸‚å ´ã«åˆã‚ã›ãŸè¨€èªãƒ»åœ°åŸŸè¨­å®šã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
    query = f"{ticker} stock" if market == "ç±³å›½æ ª (US)" else f"{ticker} æ ªä¾¡"
    url = f"https://news.google.com/rss/search?q={query}&hl={params['hl']}&gl={params['gl']}&ceid={params['ceid']}"
    
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
        root = ET.fromstring(r.text)
        headlines = [item.find('title').text for item in root.findall('.//item')[:5]]
        return headlines if headlines else ["é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"]
    except Exception:
        return ["ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ"]

def fetch_stock_data_with_fallback(ticker, suffix):
    """ã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è©¦è¡Œã™ã‚‹å …ç‰¢ãªãƒ‡ãƒ¼ã‚¿å–å¾—"""
    # æ—¥æœ¬æ ªã®å ´åˆã¯ .JP ã¨ .JPN ã®ä¸¡æ–¹ã‚’è©¦ã™ï¼ˆStooqã®æ°—ã¾ãã‚Œå¯¾ç­–ï¼‰
    suffixes = [suffix] if suffix == "US" else ["JP", "JPN"]
    
    for s in suffixes:
        url = f"https://stooq.com/q/d/l/?s={ticker.lower()}.{s.lower()}&i=d"
        try:
            r = requests.get(url, timeout=15)
            r.raise_for_status()
            df = pd.read_csv(StringIO(r.content.decode("utf-8")))
            if "Close" in df.columns and len(df) > 0:
                df["Date"] = pd.to_datetime(df["Date"])
                return df.set_index("Date").sort_index()
        except:
            continue
    raise ValueError(f"éŠ˜æŸ„ {ticker} ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

def calc_metrics(df):
    c = df["Close"].astype(float)
    rows = len(c)
    r252 = ((c.iloc[-1] / c.iloc[-252]) - 1) * 100 if rows >= 252 else None
    r21  = ((c.iloc[-1] / c.iloc[-21]) - 1) * 100 if rows >= 21 else None
    
    # æ”¹è‰¯ã•ã‚ŒãŸ 12-1ãƒ¶æœˆãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
    mom_12_1 = (r252 - r21) if (r252 is not None and r21 is not None) else None
    vol_60d = c.pct_change().rolling(60).std().iloc[-1] * (252 ** 0.5) * 100 if rows >= 62 else None
    ma_200 = c.rolling(200).mean().iloc[-1]
    ma_200_gap = ((c.iloc[-1] / ma_200) - 1) * 100 if (rows >= 200 and ma_200 != 0) else None
    window = min(252, rows)
    sub = c.iloc[-window:]
    max_dd = ((sub / sub.cummax() - 1) * 100).min()
    
    return {
        "price": round(c.iloc[-1], 2),
        "mom_12_1": round(mom_12_1, 2) if mom_12_1 is not None else None,
        "vol_60d": round(vol_60d, 2) if vol_60d is not None else None,
        "max_dd_252d": round(max_dd, 2) if max_dd is not None else None,
        "ma_200_gap": round(ma_200_gap, 2) if ma_200_gap is not None else None,
        "ret_1m": round(r21, 2) if r21 is not None else None,
    }

# --- 4. å§”å“¡ä¼šåˆè­°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæ§‹é€ åŒ–ï¼‰ ---

def run_structured_committee(analyzed_data):
    data_json = json.dumps(analyzed_data, ensure_ascii=False)
    prompt = f"""
    ã‚ãªãŸã¯æŠ•è³‡å§”å“¡ä¼šã®è­°é•·ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œå”¯ä¸€ã®äº‹å®Ÿã€ã¨ã—ã¦ã€Top3éŠ˜æŸ„ã‚’é¸å®šã—ã¦ãã ã•ã„ã€‚

    ã€åˆ¶ç´„äº‹é …ã€‘
    - æä¾›ã•ã‚ŒãŸã€Œnewsã€ä»¥å¤–ã®å¤–éƒ¨ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¨æ¸¬ã§æ›¸ã‹ãªã„ã§ãã ã•ã„ã€‚
    - ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦‹å‡ºã—ã‹ã‚‰æ¥­ç•Œäº‹æƒ…ã‚’æ·±èª­ã¿ã—ã™ããšã€è¦‹å‡ºã—ã®äº‹å®Ÿã«é™å®šã—ã¦ãã ã•ã„ã€‚
    - å„éŠ˜æŸ„ã« [Pos/Neu/Neg] ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ã—ã¦ãã ã•ã„ã€‚
    - ä¸æ˜ãªç‚¹ã¯ã€Œãƒ‡ãƒ¼ã‚¿ä¸è¶³ã«ã‚ˆã‚Šä¸æ˜ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚

    ã€ãƒ‡ãƒ¼ã‚¿ã€‘
    {data_json}

    ã€å‡ºåŠ›å½¢å¼ã€‘
    1. å„éŠ˜æŸ„ã®å€‹åˆ¥åˆ†æï¼ˆæ•°å€¤å¼•ç”¨ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã¨ãã®æ ¹æ‹ ï¼‰
    2. æœ€çµ‚Top3ãƒ©ãƒ³ã‚­ãƒ³ã‚°
    3. å…¨ä½“ã‚’é€šã—ãŸãƒªã‚¹ã‚¯ç®¡ç†ä¸Šã®æ³¨æ„ç‚¹
    """
    response = client.models.generate_content(model='gemini-flash-latest', contents=prompt)
    return response.text

# --- 5. ãƒ¡ã‚¤ãƒ³ç”»é¢ã®æŒ™å‹• ---

if st.sidebar.button("ğŸš€ ã‚°ãƒ­ãƒ¼ãƒãƒ«ç²¾æŸ»ã‚’é–‹å§‹"):
    final_list = []
    all_dfs = {}
    
    with st.status("ä¸–ç•Œå¸‚å ´ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆä¸­...", expanded=True) as status:
        for ticker in TICKERS:
            try:
                st.write(f"â³ {ticker}.{market_suffix} ã®å¤šè§’åˆ†æä¸­...")
                df = fetch_stock_data_with_fallback(ticker, market_suffix)
                m = calc_metrics(df)
                m["ticker"] = ticker
                m["news"] = fetch_news_headlines(ticker, news_params)
                
                if m["max_dd_252d"] is not None and m["max_dd_252d"] < RISK_DD_REJECT:
                    st.write(f"ğŸš« {ticker}: ãƒªã‚¹ã‚¯ï¼ˆDD {m['max_dd_252d']}%ï¼‰ãŒè¨±å®¹ç¯„å›²å¤–ã®ãŸã‚é™¤å¤–")
                    continue
                
                final_list.append(m)
                all_dfs[ticker] = df
                time.sleep(0.5)
            except Exception as e:
                st.error(f"âŒ {ticker} ã®åˆ†æã‚’ã‚¹ã‚­ãƒƒãƒ—: {e}")
        status.update(label="ç²¾æŸ»å®Œäº†", state="complete", expanded=False)

    if len(final_list) >= 1:
        report = run_structured_committee(final_list)
        
        col1, col2 = st.columns([1, 1])
        with col1:
            st.header("ğŸ† æŠ•è³‡å§”å“¡ä¼šãƒ»æœ€çµ‚è©•è­°")
            st.markdown(report)
        
        with col2:
            st.header("ğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹")
            for m in final_list:
                with st.expander(f"ğŸ“Š {m['ticker']} ({market_suffix})", expanded=True):
                    # ç°¡æ˜“ãƒãƒ£ãƒ¼ãƒˆ
                    fig = go.Figure()
                    df_plot = all_dfs[m['ticker']]
                    fig.add_trace(go.Scatter(x=df_plot.index, y=df_plot['Close'], name='Price'))
                    ma200 = df_plot['Close'].rolling(200).mean()
                    fig.add_trace(go.Scatter(x=df_plot.index, y=ma200, name='200MA', line=dict(dash='dash')))
                    fig.update_layout(height=300, margin=dict(l=0,r=0,t=0,b=0))
                    st.plotly_chart(fig, use_container_width=True)
                    
                    st.write("**æœ€æ–°ãƒ˜ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³:**")
                    for h in m["news"]:
                        st.write(f"ğŸ”¹ {h}")
                    
                    # ãƒ­ã‚°ä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
                    st.json(m)
        
        # å®Ÿè¡Œãƒ­ã‚°ã®ä¿å­˜
        run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        with open(f"log_{run_id}.json", "w", encoding="utf-8") as f:
            json.dump({"report": report, "data": final_list}, f, ensure_ascii=False, indent=2)
    else:
        st.error("åˆ†æå¯¾è±¡éŠ˜æŸ„ãŒè¶³ã‚Šã¾ã›ã‚“ã€‚")