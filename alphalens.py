import os
import time
import re
import math
import urllib.parse
import urllib.request
import traceback
import xml.etree.ElementTree as ET
import email.utils
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional

import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st
import yfinance as yf

# Import Universe
try:
    import universe
except ImportError:
    st.error("CRITICAL: 'universe.py' not found.")
    st.stop()

# --- UTILS ---
def log_system_event(msg: str, level: str = "INFO", tag: str = "SYS"):
    timestamp = datetime.now().strftime("%H:%M:%S")
    line = f"[{timestamp}] [{level}] [{tag}] {msg}"
    print(line)
    if "system_logs" in st.session_state:
        st.session_state.system_logs.append(line)
        st.session_state.system_logs = st.session_state.system_logs[-300:]

MARKETS = universe.MARKETS
NAME_DB = universe.NAME_DB
LOOKBACKS = {"1W (5d)": 5, "1M (21d)": 21, "3M (63d)": 63, "12M (252d)": 252}
FETCH_PERIOD = "24mo"

@st.cache_data(ttl=86400)
def fetch_name_fallback(ticker: str) -> str:
    try:
        info = yf.Ticker(ticker).info
        n = info.get("shortName") or info.get("longName")
        if n and isinstance(n, str) and len(n) >= 2: return n
    except: pass
    return ticker

def get_name(t: str) -> str:
    n = NAME_DB.get(t)
    if n and n != t: return n
    return fetch_name_fallback(t)

def sfloat(x):
    try: return float(x)
    except: return np.nan

def clamp(x, lo, hi):
    return lo if x < lo else hi if x > hi else x

def sentiment_label(score: int) -> str:
    if score >= 3: return "POS"
    if score <= -3: return "NEG"
    return "NEUT"

def dash(x, fmt="%.1f"):
    if pd.isna(x): return "-"
    try: return fmt % float(x)
    except: return "-"

def pct(x, fmt="%.1f"):
    if pd.isna(x): return "-"
    try: return (fmt % (float(x)*100)) + "%"
    except: return "-"

def outlook_date_slots(days: List[int] = [7, 21, 35, 49, 63, 84]) -> List[str]:
    base = datetime.now().date()
    return [(base + timedelta(days=d)).strftime("%Y/%m/%d") for d in days]

def safe_link_button(label: str, url: str, use_container_width: bool = True):
    if not url:
        st.button(label, disabled=True, use_container_width=use_container_width)
        return
    try:
        fn = getattr(st, "link_button", None)
        if callable(fn):
            fn(label, url, use_container_width=use_container_width)
        else:
            st.markdown(f"- [{label}]({url})")
    except Exception:
        st.markdown(f"- [{label}]({url})")

def build_ir_links(name: str, ticker: str, website: Optional[str], market_key: str) -> Dict[str, str]:
    q_site = urllib.parse.quote(name)
    q_ir = urllib.parse.quote(f"{name} IR")
    if "US" in market_key:
        q_deck = urllib.parse.quote(f"{name} investor presentation earnings pdf")
    else:
        q_deck = urllib.parse.quote(f"{name} Ê±∫ÁÆóË™¨ÊòéË≥áÊñô pdf")
            
    official = website if website and website.startswith("http") else f"https://www.google.com/search?q={q_site}+official+site"
    
    return {
        "official": official,
        "ir_search": f"https://www.google.com/search?q={q_ir}",
        "earnings_deck": f"https://www.google.com/search?q={q_deck}",
    }

# --- DATA FETCHING ---
@st.cache_data(ttl=1800)
def fetch_market_data(tickers: Tuple[str, ...], period: str) -> pd.DataFrame:
    tickers = tuple(dict.fromkeys([t for t in tickers if t]))
    frames = []
    chunk = 40 
    for i in range(0, len(tickers), chunk):
        c = tickers[i:i+chunk]
        try:
            r = yf.download(" ".join(c), period=period, interval="1d", group_by="ticker", auto_adjust=True, threads=True, progress=False)
            if not r.empty: frames.append(r)
        except: continue
    return pd.concat(frames, axis=1) if frames else pd.DataFrame()

def extract_close_prices(df: pd.DataFrame, expected: List[str]) -> pd.DataFrame:
    if df.empty: return pd.DataFrame()
    try:
        if isinstance(df.columns, pd.MultiIndex):
            if "Close" in df.columns.get_level_values(0): close = df.xs("Close", axis=1, level=0)
            elif "Close" in df.columns.get_level_values(1): close = df.xs("Close", axis=1, level=1)
            else: return pd.DataFrame()
        else: return pd.DataFrame()
        close = close.apply(pd.to_numeric, errors="coerce").dropna(how="all")
        return close[[c for c in expected if c in close.columns]]
    except: return pd.DataFrame()

def calc_technical_metrics(s: pd.Series, b: pd.Series, win: int) -> Dict:
    s_clean, b_clean = s.dropna(), b.dropna()
    if len(s_clean) < win + 1 or len(b_clean) < win + 1: return None
    s_win, b_win = s.ffill().tail(win+1), b.ffill().tail(win+1)
    if s_win.isna().iloc[0] or b_win.isna().iloc[0]: return None

    p_ret = (s_win.iloc[-1]/s_win.iloc[0]-1)*100
    b_ret = (b_win.iloc[-1]/b_win.iloc[0]-1)*100
    rs = p_ret - b_ret
    
    half = max(1, win//2)
    p_half = (s_win.iloc[-1]/s_win.iloc[-half-1]-1)*100
    accel = p_half - (p_ret/2)
    dd = abs(((s_win/s_win.cummax()-1)*100).min())
    year_high = s_clean.tail(252).max() if len(s_clean) >= 252 else s_clean.max()
    high_dist = (s_win.iloc[-1] / year_high - 1) * 100 if year_high > 0 else 0
    
    rets = {}
    s_ffill = s.ffill()
    for l, d in [("1W",5), ("1M",21), ("3M",63), ("12M",252)]:
        if len(s_ffill) > d: rets[l] = (s_ffill.iloc[-1] / s_ffill.iloc[-1-d] - 1) * 100
        else: rets[l] = np.nan
    
    return {"RS": rs, "Accel": accel, "MaxDD": dd, "Ret": p_ret, "HighDist": high_dist, **rets}

def calculate_regime(bench_series: pd.Series) -> Tuple[str, float]:
    if len(bench_series) < 200: return "Unknown", 0.5
    curr = bench_series.iloc[-1]
    ma200 = bench_series.rolling(200).mean().iloc[-1]
    trend = "Bull" if curr > ma200 else "Bear"
    return trend, 0.6 if trend == "Bull" else 0.3

def audit_data_availability(expected: List[str], df: pd.DataFrame, win: int):
    present = [t for t in expected if t in df.columns]
    if not present: return {"ok": False, "list": []}
    last = df[present].apply(lambda x: x.last_valid_index())
    mode = last.mode().iloc[0] if not last.mode().empty else None
    computable = [t for t in present if last[t] == mode and len(df[t].dropna()) >= win + 1]
    return {"ok": True, "list": computable, "mode": mode, "count": len(computable), "total": len(expected)}

def calculate_zscore(s: pd.Series) -> pd.Series:
    if s.std() == 0: return pd.Series(0.0, index=s.index)
    return (s - s.mean()) / s.std(ddof=0)

def price_action_pack(price: pd.Series) -> Dict[str, float]:
    p = price.dropna()
    if len(p) < 60: return {}
    out = {}
    out["Last"] = float(p.iloc[-1])
    try:
        out["1D"] = float((p.iloc[-1] / p.iloc[-2] - 1) * 100) if len(p) >= 2 else np.nan
        out["1W"] = float((p.iloc[-1] / p.iloc[-6] - 1) * 100) if len(p) >= 6 else np.nan
        out["1M"] = float((p.iloc[-1] / p.iloc[-22] - 1) * 100) if len(p) >= 22 else np.nan
        out["3M"] = float((p.iloc[-1] / p.iloc[-64] - 1) * 100) if len(p) >= 64 else np.nan
        ma200 = p.rolling(200).mean().iloc[-1] if len(p) >= 200 else np.nan
        out["200DMA_Dist"] = float((p.iloc[-1] / ma200 - 1) * 100) if pd.notna(ma200) and ma200 != 0 else np.nan
        dd = (p / p.cummax() - 1) * 100
        out["MaxDD_6M"] = float(dd.tail(126).min()) if len(dd) >= 126 else float(dd.min())
    except: pass
    return out

# --- FUNDAMENTALS ---
@st.cache_data(ttl=3600)
def fetch_fundamentals_batch(tickers: List[str]) -> pd.DataFrame:
    data = []
    def get_info(t):
        try:
            i = yf.Ticker(t).info
            pe = i.get("trailingPE", np.nan)
            if pe is not None and pe < 0: pe = np.nan
            pbr = i.get("priceToBook", np.nan)
            if pbr is not None and pbr < 0: pbr = np.nan
            return {
                "Ticker": t, "MCap": i.get("marketCap", 0),
                "PER": pe, "PBR": pbr, "FwdPE": i.get("forwardPE", np.nan),
                "ROE": i.get("returnOnEquity", np.nan),
                "OpMargin": i.get("operatingMargins", np.nan),
                "RevGrow": i.get("revenueGrowth", np.nan),
                "Beta": i.get("beta", np.nan)
            }
        except: return {"Ticker": t, "MCap": 0}
    with ThreadPoolExecutor(max_workers=10) as executor:
        data = list(executor.map(get_info, tickers))
    return pd.DataFrame(data).set_index("Ticker")

@st.cache_data(ttl=3600)
def get_fundamental_data(ticker: str) -> Dict[str, Any]:
    try:
        i = yf.Ticker(ticker).info
        pe = i.get("trailingPE", np.nan)
        if isinstance(pe, (int, float)) and pe < 0: pe = np.nan
        pbr = i.get("priceToBook", np.nan)
        if isinstance(pbr, (int, float)) and pbr < 0: pbr = np.nan
        
        return {
            "MCap": i.get("marketCap", 0), "PER": pe, "FwdPE": i.get("forwardPE", np.nan),
            "PBR": pbr, "PEG": i.get("pegRatio", np.nan), "Target": i.get("targetMeanPrice", np.nan),
            "Rec": i.get("recommendationKey", "N/A"), "Website": i.get("website", None)
        }
    except: return {"PRICE": np.nan, "MCap": np.nan, "PER": np.nan, "FwdPE": np.nan, "PBR": np.nan, "PEG": np.nan}

def pick_fund_row(cand_fund: pd.DataFrame, ticker: str) -> Dict[str, Any]:
    try:
        if cand_fund is None or cand_fund.empty: return {}
        m = cand_fund[cand_fund["Ticker"] == ticker]
        if m.empty: return {}
        return m.iloc[0].to_dict()
    except: return {}

@st.cache_data(ttl=3600)
def fetch_earnings_dates(ticker: str) -> Dict[str,str]:
    out = {}
    try:
        cal = yf.Ticker(ticker).calendar
        if cal is not None:
            if isinstance(cal, dict) and 'Earnings Date' in cal:
                out["EarningsDate"] = str(cal['Earnings Date'][0])
            elif isinstance(cal, pd.DataFrame):
                 for k in ["Earnings Date", "EarningsDate"]:
                    if k in cal.index:
                        v = cal.loc[k].values
                        out["EarningsDate"] = ", ".join([str(x)[:10] for x in v if str(x) != "nan"])
    except: pass
    return out

# --- AI & TEXT ---
API_KEY = st.secrets.get("GEMINI_API_KEY") or st.secrets.get("GOOGLE_API_KEY") or os.getenv("GOOGLE_API_KEY")
try:
    import google.generativeai as genai
    HAS_LIB = True
    if API_KEY: genai.configure(api_key=API_KEY)
except: HAS_LIB = False

def clean_ai_text(text: str) -> str:
    text = text.replace("```text", "").replace("```", "")
    text = text.replace("**", "").replace('"', "").replace("'", "")
    text = re.sub(r"(?m)^\s*text\s*$", "", text)
    text = re.sub(r"(?m)^\s*#{2,}\s*", "", text)
    text = re.sub(r"(?im)^\s*(agent|„Ç®„Éº„Ç∏„Çß„É≥„Éà)\s*[A-E0-9]+[:Ôºö]\s*", "", text)
    # strip polite / meta preambles
    text = re.sub(r"(?m)^\s*(„ÅØ„ÅÑ„ÄÅ)?\s*ÊâøÁü•(„ÅÑ„Åü„Åó„Åæ„Åó„Åü|„Åó„Åæ„Åó„Åü)„ÄÇ?.*$\n?", "", text)
    text = re.sub(r"(?m)^\s*‰ª•‰∏ã„Å´(.*)‰ΩúÊàê(„Åô„Çã|„Åó„Åæ„Åô)„ÄÇ?.*$\n?", "", text)
    text = re.sub(r"(?m)^\s*„Åî‰æùÈ†º(.*)„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åô„ÄÇ?.*$\n?", "", text)
    bad = ["‰∏çÊòé", "„Çè„Åã„Çâ„Å™„ÅÑ", "ÂàÜ„Åã„Çâ„Å™„ÅÑ", "unknown"]
    for w in bad: text = re.sub(rf"(?m)^.*{re.escape(w)}.*$\n?", "", text)
    return re.sub(r"\n{2,}", "\n", text).strip()

def force_nonempty_outlook_market(text: str, trend: str, ret: float, spread: float, market_key: str) -> str:
    m = re.search(r"„Äê‰ªäÂæå3„É∂Êúà[^„Äë]*„Äë\n?(.*)", text, flags=re.DOTALL)
    body = m.group(1).strip() if m else ""
    if len(re.sub(r"[\s\(\)„Éª\-‚àí\n]", "", body)) >= 30: return text

    slots = outlook_date_slots()
    if "US" in market_key:
        events = [
            f"FOMC({slots[1]})‚ÜíÈáëÂà©Áπî„ÇäËæº„ÅøÂÜçË®àÁÆó„Åß„Éè„Ç§PER„ÅÆÂ§âÂãï„ÅåÂ¢óÂπÖ",
            f"CPI/PCE({slots[0]})‚Üí„Ç§„É≥„Éï„É¨ÈàçÂåñ„Å™„Çâ„É™„Çπ„ÇØ„Ç™„É≥„ÄÅÂÜçÂä†ÈÄü„Å™„Çâ„É™„Çπ„ÇØ„Ç™„Éï",
            f"ÈõáÁî®Áµ±Ë®à({slots[0]})‚ÜíË≥ÉÈáë„ÅÆÁ≤òÁùÄÊÄß„ÅåÈï∑ÊúüÈáëÂà©„ÇíÂ∑¶Âè≥",
            f"‰∏ªË¶ÅÊ±∫ÁÆó({slots[2]})‚Üí„Ç¨„Ç§„ÉÄ„É≥„Çπ„ÅßÊåáÊï∞ÂØÑ‰∏é„ÅåÈõÜ‰∏≠„Åó„ÇÑ„Åô„ÅÑ",
            f"„ÇØ„É¨„Ç∏„ÉÉ„Éà/ÊµÅÂãïÊÄß({slots[3]})‚Üí„Çπ„Éó„É¨„ÉÉ„ÉâÊã°Â§ß„ÅØÊ†™„ÅÆ‰∏äÂÄ§ÊäëÂà∂",
            f"ÈúÄÁµ¶„Ç§„Éô„É≥„Éà({slots[4]})‚Üí„Ç™„Éó„Ç∑„Éß„É≥„Éª„É™„Éê„É©„É≥„Çπ„ÅßÁü≠Êúü„Çπ„Éë„Ç§„ÇØ"
        ]
    else:
        events = [
            f"Êó•ÈäÄ‰ºöÂêà({slots[1]})‚ÜíÈáëÂà©„Å®ÂÜÜ„ÅåÂêåÊôÇ„Å´Âãï„Åç„ÄÅÂ§ñÈúÄ/ÂÜÖÈúÄ„ÅÆÂÑ™Âä£„ÅåÂèçËª¢„Åó„ÇÑ„Åô„ÅÑ",
            f"Á±≥ÈáëÂà©„ÉªÂÜÜÁõ∏Â†¥({slots[0]})‚ÜíËº∏Âá∫„Éª„Ç§„É≥„Éê„Ç¶„É≥„Éâ„ÅÆÊÑüÂøúÂ∫¶„ÅåÈ´ò„ÅÑ",
            f"‰∏ªË¶ÅÊ±∫ÁÆó({slots[2]})‚ÜíÈÄöÊúüË¶ãÈÄö„Åó‰øÆÊ≠£„Å®Ê†™‰∏ªÈÇÑÂÖÉ„ÅåÈúÄÁµ¶„ÇíÊ±∫„ÇÅ„Çã",
            f"ÊåáÊï∞„É™„Éê„É©„É≥„Çπ({slots[3]})‚ÜíÈúÄÁµ¶Ê≠™„Åø„ÅßÁü≠ÊúüÂ§âÂãï„ÅåÂá∫„ÇÑ„Åô„ÅÑ",
            f"Ë≥É‰∏ä„Åí„ÉªÁâ©‰æ°({slots[4]})‚ÜíÂÆüË≥™Ë≥ÉÈáë„ÅßÊ∂àË≤ªÈñ¢ÈÄ£„ÅÆÁõ∏ÂØæ„ÅåÂãï„Åè",
            f"Êµ∑Â§ñÊäïË≥áÂÆ∂„Éï„É≠„Éº({slots[5]})‚ÜíË≥áÈáëÊµÅÂÖ•„ÅÆÁ∂ôÁ∂öÊÄß„ÅåÂú∞Âêà„ÅÑ„ÇíË¶èÂÆö"
        ]

    fallback = "„Äê‰ªäÂæå3„É∂Êúà„ÅÆ„Ç≥„É≥„Çª„É≥„Çµ„ÇπË¶ãÈÄö„Åó„Äë\n" + "\n".join([f"„Éª{e}" for e in events]) + \
               f"\n„ÉªÂº∑Ê∞óÊù°‰ª∂Ôºö„Ç§„É≥„Éï„É¨ÈéÆÈùôÂåñÔºãÊ•≠Á∏æ„Ç¨„Ç§„ÉÄ„É≥„Çπ‰∏äÊåØ„ÇåÔºàÂü∫Ë™ø:{trend}Ôºâ\n„ÉªÂº±Ê∞óÊù°‰ª∂ÔºöÈáëÂà©ÂÜç‰∏äÊòáÔºã„Ç¨„Ç§„ÉÄ„É≥„Çπ‰∏ãÊñπ‰øÆÊ≠£„ÅÆÈÄ£Èéñ"

    if "„Äê‰ªäÂæå3„É∂Êúà" in text:
        text = re.sub(r"„Äê‰ªäÂæå3„É∂Êúà[^„Äë]*„Äë.*", fallback, text, flags=re.DOTALL)
    else:
        text = text.rstrip() + "\n" + fallback
    return text

def enforce_market_format(text: str) -> str:
    """Normalize Market Pulse text to required sections; resilient to messy LLM outputs."""
    if not isinstance(text, str):
        text = str(text)

    text = text.replace("\r\n", "\n").replace("\r", "\n").strip()

    # Remove common assistant boilerplate/meta
    text = re.sub(r"(?im)^\s*(„ÅØ„ÅÑ„ÄÅ)?\s*ÊâøÁü•(„ÅÑ„Åü)?„Åó„Åæ„Åó„Åü[„ÄÇ!ÔºÅ]*.*\n+", "", text)
    text = re.sub(r"(?im)^\s*‰ª•‰∏ã„Å´.*(‰ΩúÊàê|ÁîüÊàê).*(„Åó„Åæ„Åô|„ÅÑ„Åü„Åó„Åæ„Åô)[„ÄÇ!ÔºÅ]*\s*$", "", text)

    # Remove unwanted date suffix right after the outlook header
    text = re.sub(r"(„Äê‰ªäÂæå3„É∂Êúà[^„Äë]*„Äë)\s*\(\d{4}[-/]\d{2}[-/]\d{2}\)", r"\1", text)
    text = re.sub(r"(„Äê‰ªäÂæå3„É∂Êúà[^„Äë]*„Äë)\s*\d{4}[-/]\d{2}[-/]\d{2}", r"\1", text)

    # Ensure required headers exist
    if "„ÄêÂ∏ÇÂ†¥Ê¶ÇÊ≥Å„Äë" not in text:
        text = "„ÄêÂ∏ÇÂ†¥Ê¶ÇÊ≥Å„Äë\n" + text

    if "„Äê‰∏ª„Å™Â§âÂãïË¶ÅÂõ†„Äë" not in text:
        text += "\n\n„Äê‰∏ª„Å™Â§âÂãïË¶ÅÂõ†„Äë\n(+) ‰∏äÊòáË¶ÅÂõ†:\n(-) ‰∏ãËêΩË¶ÅÂõ†:"

    if "„Äê‰ªäÂæå3„É∂Êúà" not in text:
        text += "\n\n„Äê‰ªäÂæå3„É∂Êúà„ÅÆ„Ç≥„É≥„Çª„É≥„Çµ„ÇπË¶ãÈÄö„Åó„Äë\n"

    return text

def enforce_index_naming(text: str, index_label: str) -> str:
    if not index_label:
        return text
    # Replace vague wording with explicit index label
    text = re.sub(r"Â∏ÇÂ†¥Âπ≥Âùá(„É™„Çø„Éº„É≥)?", index_label, text)
    text = re.sub(r"ÊåáÊï∞(?:ÂÖ®‰Ωì)?", index_label, text)
    # Ensure the index label appears at least once in the market overview
    if index_label not in text and "„ÄêÂ∏ÇÂ†¥Ê¶ÇÊ≥Å„Äë" in text:
        text = re.sub(r"(„ÄêÂ∏ÇÂ†¥Ê¶ÇÊ≥Å„Äë\n?)", rf"\1{index_label}„ÇíÂü∫Ê∫ñ„Å´Ë®òËø∞„Åô„Çã„ÄÇ\n", text, count=1)
    return text

def group_plus_minus_blocks(text: str) -> str:
    # Extract the block
    m = re.search(r"„Äê‰∏ª„Å™Â§âÂãïË¶ÅÂõ†„Äë\n?(.*?)(?=\n„Äê|\Z)", text, flags=re.DOTALL)
    if not m:
        return text
    block = m.group(1).strip()
    lines = [l.strip() for l in block.splitlines() if l.strip()]

    # remove fake headings that often get bulletized
    heading_trash = {"‰∏äÊòáË¶ÅÂõ†:", "‰∏ãËêΩË¶ÅÂõ†:", "(+) ‰∏äÊòáË¶ÅÂõ†:", "(-) ‰∏ãËêΩË¶ÅÂõ†:", "Ôºà+Ôºâ‰∏äÊòáË¶ÅÂõ†:", "Ôºà‚àíÔºâ‰∏ãËêΩË¶ÅÂõ†:"}
    cleaned = []
    for l in lines:
        l2 = l.lstrip("-„Éª ").strip()
        if l2 in heading_trash:
            continue
        # remove "„Ç§„Éô„É≥„ÉàA" etc accidentally placed here
        if l2.startswith("3)") or "‰ªäÂæå3„É∂Êúà" in l2:
            continue
        cleaned.append(l)

    pos, neg, oth = [], [], []
    pos_kw = ["‰∏äÊñπ‰øÆÊ≠£","Â¢óÁõä","Â•ΩË™ø","ÂõûÂæ©","‰Ωé‰∏ã","ÈàçÂåñ","Âà©‰∏ã„Åí","Ë≤∑„ÅÑ","Ë≥áÈáëÊµÅÂÖ•","Âº∑„ÅÑ","‰∏äÊòá","ÊîπÂñÑ","Ââ≤ÂÆâ","Ëá™Á§æÊ†™Ë≤∑„ÅÑ","ÈúÄË¶ÅÂ¢ó","ÂèóÊ≥®Â¢ó"]
    neg_kw = ["‰∏ãÊñπ‰øÆÊ≠£","Ê∏õÁõä","ÊÇ™Âåñ","Â§±ÈÄü","ÂÜçÂä†ÈÄü","Âà©‰∏ä„Åí","Âºï„ÅçÁ∑†„ÇÅ","Â£≤„Çä","Ë≥áÈáëÊµÅÂá∫","‰∏ãËêΩ","Ë≠¶Êàí","È´òÊ≠¢„Åæ„Çä","„É™„Çπ„ÇØ","Âú∞ÊîøÂ≠¶","Èï∑ÊúüÈáëÂà©‰∏äÊòá","„Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£","Êá∏Âøµ"]

    for l in cleaned:
        raw = l.lstrip("-„Éª ").strip()
        # explicit sign markers
        if raw.startswith("(+)") or raw.startswith("Ôºã") or raw.startswith("+"):
            pos.append(raw.lstrip("()+Ôºã+ ").strip())
            continue
        if raw.startswith("(-)") or raw.startswith("‚àí") or raw.startswith("-"):
            neg.append(raw.lstrip("()-‚àí- ").strip())
            continue
        # keyword routing
        score = 0
        if any(k in raw for k in pos_kw): score += 1
        if any(k in raw for k in neg_kw): score -= 1
        if score > 0:
            pos.append(raw)
        elif score < 0:
            neg.append(raw)
        else:
            oth.append(raw)

    # Build normalized section
    def bullets(arr):
        return "\n".join([f"- {x}" for x in arr[:6]]) if arr else "- ÔºàË©≤ÂΩìÊùêÊñô„ÇíÊäΩÂá∫„Åß„Åç„ÅöÔºâ"
    out = "„Äê‰∏ª„Å™Â§âÂãïË¶ÅÂõ†„Äë\n(+) ‰∏äÊòáË¶ÅÂõ†:\n" + bullets(pos) + "\n(‚àí) ‰∏ãËêΩË¶ÅÂõ†:\n" + bullets(neg)
    if oth:
        out += "\n(Ë£úË∂≥):\n" + "\n".join([f"- {x}" for x in oth[:6]])
    # Replace original block
    return text[:m.start()] + out + text[m.end():]
def enforce_da_dearu_soft(text: str) -> str:
    text = re.sub(r"„Åß„Åô„ÄÇ", "„Å†„ÄÇ", text)
    text = re.sub(r"„Åß„Åô$", "„Å†", text, flags=re.MULTILINE)
    text = re.sub(r"„Åæ„Åô„ÄÇ", "„Åô„Çã„ÄÇ", text)
    text = re.sub(r"„Åæ„Åô$", "„Åô„Çã", text, flags=re.MULTILINE)
    return text

def market_to_html(text: str) -> str:
    text = re.sub(r"(^\(\+\s*\).*$)", r"<span class='hl-pos'>\1</span>", text, flags=re.MULTILINE)
    text = re.sub(r"(^\(\-\s*\).*$)", r"<span class='hl-neg'>\1</span>", text, flags=re.MULTILINE)
    return text.replace("\n", "<br>")

@st.cache_data(ttl=1800)
def get_news_consolidated(ticker: str, name: str, market_key: str, limit_each: int = 10) -> Tuple[List[dict], str, int, Dict[str,int]]:
    news_items, context_lines = [], []
    pos_words = ["Â¢óÁõä", "ÊúÄÈ´òÂÄ§", "Â•ΩÊÑü", "‰∏äÊòá", "Ëá™Á§æÊ†™Ë≤∑„ÅÑ", "‰∏äÊñπ‰øÆÊ≠£", "ÊÄ•È®∞", "beat", "high", "jump", "record"]
    neg_words = ["Ê∏õÁõä", "ÂÆâÂÄ§", "Â´åÊ∞ó", "‰∏ãËêΩ", "‰∏ãÊñπ‰øÆÊ≠£", "ÊÄ•ËêΩ", "Ëµ§Â≠ó", "miss", "low", "drop", "warn"]
    sentiment_score = 0
    meta = {"yahoo":0, "google":0, "pos":0, "neg":0}

    # Yahoo
    try:
        raw = yf.Ticker(ticker).news or []
        for n in raw[:limit_each]:
            t, l, p = n.get("title",""), n.get("link",""), n.get("providerPublishTime",0)
            news_items.append({"title":t, "link":l, "pub":p, "src":"Yahoo"})
            if t:
                meta["yahoo"] += 1
                dt = datetime.fromtimestamp(p).strftime("%Y/%m/%d") if p else "-"
                weight = 2 if (time.time() - p) < 172800 else 1
                context_lines.append(f"- [Yahoo {dt}] {t}")
                if any(w in t for w in pos_words): sentiment_score += 1*weight; meta["pos"] += 1
                if any(w in t for w in neg_words): sentiment_score -= 1*weight; meta["neg"] += 1
    except: pass

    # Google
    try:
        if "US" in market_key:
            hl, gl, ceid = "en", "US", "US:en"
            q = urllib.parse.quote(f"{name} stock")
        else:
            hl, gl, ceid = "ja", "JP", "JP:ja"
            q = urllib.parse.quote(f"{name} Ê†™")
            
        url = f"https://news.google.com/rss/search?q={q}&hl={hl}&gl={gl}&ceid={ceid}"
        with urllib.request.urlopen(url, timeout=3) as r:
            root = ET.fromstring(r.read())
            for i in root.findall(".//item")[:limit_each]:
                t, l, d = i.findtext("title"), i.findtext("link"), i.findtext("pubDate")
                try: pub = int(email.utils.parsedate_to_datetime(d).timestamp())
                except: pub = 0
                news_items.append({"title":t, "link":l, "pub":pub, "src":"Google"})
                if t:
                    meta["google"] += 1
                    dt = datetime.fromtimestamp(pub).strftime("%Y/%m/%d") if pub else "-"
                    weight = 2 if (time.time() - pub) < 172800 else 1
                    context_lines.append(f"- [Google {dt}] {t}")
                    if any(w in t for w in pos_words): sentiment_score += 1*weight; meta["pos"] += 1
                    if any(w in t for w in neg_words): sentiment_score -= 1*weight; meta["neg"] += 1
    except: pass

    # Free public RSS feeds (fallback / enrichment). English-only is OK.
    try:
        rss_sources = [
            ("Reuters Markets", "https://feeds.reuters.com/reuters/marketsNews"),
            ("Reuters Business", "https://feeds.reuters.com/reuters/businessNews"),
            ("MarketWatch", "https://feeds.marketwatch.com/marketwatch/topstories/"),
            ("CNBC", "https://www.cnbc.com/id/100003114/device/rss/rss.html"),
            ("BBC Business", "https://feeds.bbci.co.uk/news/business/rss.xml"),
        ]
        for src, url2 in rss_sources:
            try:
                with urllib.request.urlopen(url2, timeout=3) as r:
                    root = ET.fromstring(r.read())
                    for it in root.findall('.//item')[: max(3, limit_each//3) ]:
                        t2, l2, d2 = it.findtext('title'), it.findtext('link'), it.findtext('pubDate')
                        try: pub2 = int(email.utils.parsedate_to_datetime(d2).timestamp())
                        except: pub2 = 0
                        if not t2: continue
                        news_items.append({"title": t2, "link": l2, "pub": pub2, "src": src})
                        dt2 = datetime.fromtimestamp(pub2).strftime('%Y/%m/%d') if pub2 else "-"
                        weight = 2 if (pub2 and (time.time() - pub2) < 172800) else 1
                        context_lines.append(f"- [{src} {dt2}] {t2}")
                        if any(w in t2 for w in pos_words): sentiment_score += 1*weight; meta["pos"] += 1
                        if any(w in t2 for w in neg_words): sentiment_score -= 1*weight; meta["neg"] += 1
            except Exception:
                pass
    except Exception:
        pass


    news_items.sort(key=lambda x: x["pub"], reverse=True)
    return news_items, "\n".join(context_lines[:15]), sentiment_score, meta

def temporal_sanity_flags(text: str) -> List[str]:
    bad = ["Âπ¥Êú´Âπ¥Âßã", "„ÇØ„É™„Çπ„Éû„Çπ", "Â§è‰ºë„Åø", "„ÅäÁõÜ", "Êù•Âπ¥", "Êò®Âπ¥Êú´"]
    return [w for w in bad if w in text]

def sector_debate_quality_ok(text: str) -> bool:
    needed = ["[SECTOR_OUTLOOK]", "[FUNDAMENTAL]", "[SENTIMENT]", "[VALUATION]", "[SKEPTIC]", "[RISK]", "[JUDGE]"]
    if any(t not in text for t in needed): return False
    min_chars = {
        "[SECTOR_OUTLOOK]": 220, "[FUNDAMENTAL]": 260, "[SENTIMENT]": 260,
        "[VALUATION]": 220, "[SKEPTIC]": 220, "[RISK]": 220, "[JUDGE]": 520,
    }
    for k, mn in min_chars.items():
        m = re.search(re.escape(k) + r"(.*?)(?=\n\[[A-Z_]+\]|\Z)", text, flags=re.DOTALL)
        if not m or len(re.sub(r"\s+", "", m.group(1))) < mn: return False
    if re.search(r"(?im)(ÁßÅ„ÅØ„Ç®„Éº„Ç∏„Çß„É≥„Éà|ÂÉï„ÅØ„Ç®„Éº„Ç∏„Çß„É≥„Éà|‰ø∫„ÅØ„Ç®„Éº„Ç∏„Çß„É≥„Éà|„Ç®„Éº„Ç∏„Çß„É≥„Éà[A-E])", text): return False
    return True

@st.cache_data(ttl=3600)
def generate_ai_content(prompt_key: str, context: Dict) -> str:
    if not HAS_LIB or not API_KEY: return "AI OFFLINE"
    
    models = ["gemini-2.0-flash", "gemini-2.0-flash-lite"]
    p = ""
    market_n = context.get('market_name', 'Global')
    today_str = datetime.now().strftime('%YÂπ¥%mÊúà%dÊó•')
    # slot_line: candidate dates for the next 3 months (used in market prompt)
    slot_line = context.get("slot_line")
    if not slot_line:
        # fallback: today + 7d steps (within 90 days)
        base = datetime.now().date()
        slots = [base + timedelta(days=d) for d in [7,14,21,28,35,42,49,56,63,70,77,84]]
        slot_line = ", ".join([s.strftime("%Y-%m-%d") for s in slots])
    
    
    if prompt_key == "market":
        p = f"""
        ÁèæÂú®: {today_str} („Åì„ÅÆÊó•‰ªò„ÇíÂü∫Ê∫ñ„Å´ÂàÜÊûê„Åõ„Çà)
        ÂØæË±°Â∏ÇÂ†¥: {market_n} („Åì„Çå‰ª•Â§ñ„ÅÆÂ∏ÇÂ†¥„ÅÆË©±„ÅØÁ¶ÅÊ≠¢)
        ÂØæË±°ÊåáÊï∞: {context.get('index_label','')}Ôºà„Åì„ÅÆÊåáÊï∞Âêç„ÇíÂøÖ„ÅöÊú¨Êñá„Å´ÊòéË®ò„Åõ„Çà„ÄÇ„ÄåÂ∏ÇÂ†¥Âπ≥Âùá„Äç„Å®„ÅÑ„ÅÜË™û„ÅØÁ¶ÅÊ≠¢Ôºâ
        ÊúüÈñì:{context['s_date']}„Äú{context['e_date']}
        ÂØæË±°ÊåáÊï∞„É™„Çø„Éº„É≥:{context['ret']:.2f}%
        ÊúÄÂº∑:{context['top']} ÊúÄÂº±:{context['bot']}
        „Éã„É•„Éº„Çπ:{context['headlines']}
        Nonce:{context.get('nonce',0)}
        
        „Åì„ÅÆÊúüÈñì„ÅÆ{market_n}Â∏ÇÂ†¥Ê¶ÇÊ≥Å„Çí„Éó„É≠Âêë„Åë„Å´450-650Â≠ó„ÅßË®òËø∞„Åõ„Çà„ÄÇ
        Á¶ÅÊ≠¢: „ÄåÂ∏ÇÂ†¥Âπ≥Âùá„Äç„Äå‰∏ÄËà¨Ë´ñ„Äç„ÄåÊßòÂ≠êË¶ã„Äç„Äå‰∏çÈÄèÊòé„Äç„ÄåÊ≥®Ë¶ñ„Äç„Å™„Å©„ÅÆÊäΩË±°Ë™û„ÄÇ
        ÊÆµËêΩÈñì„ÅÆÁ©∫Ë°åÁ¶ÅÊ≠¢„ÄÇÊîπË°å„ÅØË®±ÂèØ„Åô„Çã„ÅåÈÄ£Á∂öÊîπË°åÁ¶ÅÊ≠¢„ÄÇ
        
        ÂøÖ„ÅöÊ¨°„ÅÆÈ†ÜÁï™„ÅßÂá∫Âäõ„Åõ„ÇàÔºàË¶ãÂá∫„Åó„ÅØÂõ∫ÂÆöÔºâÔºö
        1) „ÄêÂ∏ÇÂ†¥Ê¶ÇÊ≥Å„ÄëÔºàÊñáÁ´†„ÅßË®òËø∞„ÄÇÁÆáÊù°Êõ∏„ÅçÁ¶ÅÊ≠¢„ÄÇÊùêÊñô‚ÜíÁµêÊûú„ÇíÂõ†Êûú„Åß„ÄÅÊï∞ÂÄ§ÂøÖÈ†à„ÄÇÊåáÊï∞Âêç={context.get('index_label','')}„ÇíÊú¨Êñá„Å´ÂøÖ„ÅöÂÖ•„Çå„ÇãÔºâ
        2) „Äê‰∏ª„Å™Â§âÂãïË¶ÅÂõ†„Äë
           (+) ‰∏äÊòáË¶ÅÂõ†: ...
           (-) ‰∏ãËêΩË¶ÅÂõ†: ...
           („Éó„É©„Çπ„Å®„Éû„Ç§„Éä„Çπ„Çí„Ç∞„É´„Éº„ÉóÂåñ„Åó„Å¶Ë®òËø∞)
        3) „Äê‰ªäÂæå3„É∂Êúà„ÅÆ„Ç≥„É≥„Çª„É≥„Çµ„ÇπË¶ãÈÄö„Åó„Äë
        - ‰∫àÂÆöÊó•„ÅØÂøÖ„ÅöÊ¨°„ÅÆÂÄôË£úÊó•„Åã„ÇâÈÅ∏„Çì„ÅßÊõ∏„ÅëÔºö{slot_line}
        - 90Êó•‰ª•ÂÜÖ„Å´Ëµ∑„Åç„ÇÑ„Åô„ÅÑÂÖ∑‰Ωì„Ç§„Éô„É≥„Éà/‰∫àÂÆö„ÇíÊúÄÂ§ß6„Å§ÂàóÊåôÔºàÊó•‰ªò„ÇÇÊÉ≥ÂÆö„Åõ„ÇàÔºâ
        - ÂêÑË°å„ÅØ„Äå„Ç§„Éô„É≥„ÉàÂêç(ÊôÇÊúü)‚ÜíÊ†™‰æ°„Å´Âäπ„Åç„ÇÑ„Åô„ÅÑÊñπÂêë‚ÜíÁêÜÁî±„Äç
        - ÊúÄÂæå„Å´Âº∑Ê∞ó/Âº±Ê∞ó„ÅÆÊù°‰ª∂ÂàÜÂ≤ê
        - „Åì„ÅÆÊúüÈñì„Åã„ÇâÂ§ñ„Çå„ÇãÂ≠£ÁØÄË°®ÁèæÔºàÂπ¥Êú´Âπ¥Âßã„ÄÅÊù•Âπ¥„Å™„Å©Ôºâ„ÅØÁ¶ÅÊ≠¢
        """
    elif prompt_key == "sector_debate_fast":
        p = f"""
        ÁèæÂú®: {today_str}
        „ÅÇ„Å™„Åü„ÅØ5Âêç„ÅÆÂ∞ÇÈñÄ„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÄÇÂØæË±°Â∏ÇÂ†¥„ÅØ{market_n}„ÄÇ
        ÂØæË±°„Çª„ÇØ„Çø„Éº:{context["sec"]}
        „Çª„ÇØ„Çø„ÉºÁµ±Ë®à:{context.get("sector_stats","")}
        „Éà„ÉÉ„ÉóÂÄôË£ú(ÂÆöÈáè/„É¢„É°„É≥„Çø„É†‰∏≠ÂøÉ):
        {context.get("top","")}
        „Éã„É•„Éº„ÇπÔºàÂøÖ„ÅöÊ†πÊã†„Å´‰Ωø„ÅÜ„ÄÇÁõ¥ËøëÂÑ™ÂÖàÔºâ:
        {context.get("news","")}
        Nonce:{context.get("nonce",0)}

        Âé≥ÂÆà:
        - Êñá‰Ωì„ÅØ„Äå„Å†„Éª„Åß„ÅÇ„Çã„Äç„ÄÇËá™Â∑±Á¥π‰ªã„ÄÅÊâøÁü•„Åó„Åæ„Åó„ÅüÁ≠â„ÅÆÂâçÁΩÆ„Åç„ÅØÁ¶ÅÊ≠¢„ÄÇ
        - 3„É∂Êúà„ÅßÊúÄ„ÇÇ‰∏ä„Åå„ÇãÁ¢∫Â∫¶„ÅåÈ´ò„ÅÑ1ÈäòÊüÑ„Å†„Åë„ÇíÊé®Â•®ÂØæË±°„Å´„Åô„Çã„ÄÇ
        - ÈáçË¶ñÈ†Ü: Áõ¥Ëøë„Éã„É•„Éº„Çπ/Ê†™‰æ°„É¢„É°„É≥„Çø„É†(1M/3M/RS) Ôºû „É™„Çπ„ÇØ(ÊúÄÂ§ßDD/È´òÂÄ§‰πñÈõ¢) Ôºû „Éê„É™„É•„Ç®„Éº„Ç∑„Éß„É≥„ÄÇ
        - ÊäΩË±°Ë™ûÔºà‰∏çÈÄèÊòé„ÄÅÂ†ÖË™ø„ÄÅÊ≥®Ë¶ñ„ÄÅÊßòÂ≠êË¶ãÔºâÁ¶ÅÊ≠¢„ÄÇÊï∞ÂÄ§„Å®Âõ†Êûú„ÅßÊõ∏„Åè„ÄÇ

        Âá∫ÂäõÔºà„Çø„Ç∞Âõ∫ÂÆö„ÄÅÂÖ®‰Ωì„Åß600„Äú900Â≠óÁõÆÂÆâÔºâ:
        [SECTOR_OUTLOOK] „Çª„ÇØ„Çø„ÉºÂÖ®‰Ωì„ÅÆ3„É∂ÊúàË¶ãÈÄö„ÅóÔºà3„Äú5ÊñáÔºâ
        [TOP_PICK] Êé®Â•®ÈäòÊüÑÔºà„ÉÜ„Ç£„ÉÉ„Ç´„ÉºÂê´„ÇÄÔºâ„Å®„ÄÅ„Å™„Åú‰ªä„Åù„Çå„Åå‰∏ä„Åå„Çä„ÇÑ„Åô„ÅÑ„ÅãÔºà5„Äú7Êñá„ÄÇ„Éã„É•„Éº„Çπ„ÇíÂ∞ë„Å™„Åè„Å®„ÇÇ2Êú¨Ê†πÊã†„Å´„Åô„ÇãÔºâ
        [RISK_TRIGGERS] 3„Å§Ôºà‰Ωï„ÅåËµ∑„Åç„Çã„Å®Â§ñ„Çå„Çã„Åã/‰∏ã„Åå„Çã„ÅãÔºâ
        [JUDGE] ÁµêË´ñ„Çí1Êñá„ÅßÊñ≠ÂÆöÔºàË≤∑„ÅÑ/Ë¶ãÈÄÅ„ÇäÁ≠âÔºâ„ÄÅÊ¨°„Å´Ë¶ã„Çã„Åπ„Åç1ÊåáÊ®ô„Çí1„Å§„Å†„Åë„ÄÇ
        """
    elif prompt_key == "sector_debate":
        p = f"""
        ÁèæÂú®: {today_str}
        „ÅÇ„Å™„Åü„ÅØ5Âêç„ÅÆÂ∞ÇÈñÄ„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÄÇÂØæË±°Â∏ÇÂ†¥„ÅØ{market_n}„ÄÇ
        ÂØæË±°„Çª„ÇØ„Çø„Éº:{context['sec']}
        ÂÄôË£ú„Éá„Éº„ÇøÔºàÂøÖ„ÅöÊØîËºÉ„Åß‰Ωø„ÅÜÔºâ:
        {context['candidates']}
        „Éã„É•„Éº„ÇπÔºàÈùûÊßãÈÄ†„ÄÅÂøÖ„ÅöÂºïÁî®„Åó„Å¶Ê†πÊã†ÂåñÔºâ:
        {context.get('news','')}
        Nonce:{context.get('nonce',0)}

        Âé≥ÂÆà„É´„Éº„É´:
        - Êñá‰Ωì„ÅØ„Äå„Å†„Éª„Åß„ÅÇ„Çã„Äç„ÄÇ„Åß„Åô„Éª„Åæ„ÅôË™ø„ÅØÁ¶ÅÊ≠¢„ÄÇ
        - ÂêÑ„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅØÊúÄ‰Ωé8Ë°å‰ª•‰∏ä„ÄÇÁü≠ÊñáÁ¶ÅÊ≠¢„ÄÇÂÖ∑‰Ωì„ÅßÊõ∏„Åè„ÄÇ
        - ÂÆöÈáè„ÅÆÂÑ™ÂÖàÈ†Ü‰Ωç„ÅØ„Äå„É¢„É°„É≥„Çø„É†/„Çª„É≥„ÉÅ„É°„É≥„ÉàÔºû„Éê„É™„É•„Ç®„Éº„Ç∑„Éß„É≥Ôºû„Éï„Ç°„É≥„ÉÄ„Äç„Åß„ÅÇ„Çã„ÄÇ
        - „ÄåÊäΩË±°Ë™ûÔºà‰∏çÈÄèÊòé„ÄÅÂ†ÖË™ø„ÄÅÊ≥®Ë¶ñ„ÄÅÊßòÂ≠êË¶ãÔºâ„Äç„ÅØÁ¶ÅÊ≠¢„ÄÇÂøÖ„Åö‰Ωï„ÅåËµ∑„Åç„Çã„Å®„Å©„ÅÜÂãï„Åè„Åã„ÇíÊõ∏„Åè„ÄÇ

        „Çø„Çπ„ÇØ:
        1) „Åæ„ÅöÂÜíÈ†≠„Å´[SECTOR_OUTLOOK]„Çø„Ç∞„Åß„ÄÅ„Çª„ÇØ„Çø„ÉºÂÖ®‰Ωì„ÅÆË¶ãÈÄö„ÅóÔºà{today_str}„Åã„Çâ3„É∂ÊúàÔºâ„ÇíÂÆ£Ë®ÄÊäú„Åç„ÅßË®òËø∞„ÄÇ
        2) „Åù„ÅÆÂæå„ÄÅÂêÑ„Ç®„Éº„Ç∏„Çß„É≥„Éà„Åå„ÄÅÂÜíÈ†≠1Êñá„Åß„Çª„ÇØ„Çø„ÉºË¶ãÈÄö„Åó„ÇíËø∞„Åπ„Åü„ÅÜ„Åà„Åß„ÄÅÂÄôË£ú„ÇíÊØîËºÉ„ÅóÁµêË´ñ„ÇíÊõ∏„Åè„ÄÇ
        
        [JUDGE]„Åß„ÅØ„ÄÅ„Éà„ÉÉ„Éó„Éî„ÉÉ„ÇØ1ÈäòÊüÑ„Å®Ê¨°ÁÇπ2ÈäòÊüÑ„ÇíÊ±∫ÂÆö„Åó„ÄÅ„Åù„ÅÆË´ñÁêÜÁöÑÊ†πÊã†„ÇíË©≥Á¥∞ÔºàÂæìÊù•„ÅÆ5ÂÄç„ÅÆÂàÜÈáèÔºâ„Å´Ë®òËø∞„Åõ„Çà„ÄÇ
        „Éç„Ç¨„ÉÜ„Ç£„Éñ„Å™ÈäòÊüÑ„Åå„ÅÇ„Çå„Å∞ÂÖ∑‰ΩìÁöÑ„Å´ÊåáÊëò„Åõ„Çà„ÄÇ
        
        Âá∫Âäõ„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÔºà„Çø„Ç∞Âé≥ÂÆàÔºâ:
        [SECTOR_OUTLOOK] ...
        [FUNDAMENTAL] ...
        [SENTIMENT] ...
        [VALUATION] ...
        [SKEPTIC] ...
        [RISK] ...
        [JUDGE] ...
        """
    elif prompt_key == "sector_report":
        p = f"""
        ÁèæÂú®: {today_str}
        ÂØæË±°Â∏ÇÂ†¥: {market_n}
        ÂØæË±°„Çª„ÇØ„Çø„Éº: {context['sec']}
        ÊúüÈñì:{context['s_date']}„Äú{context['e_date']}
        „Çª„ÇØ„Çø„ÉºÁµ±Ë®à: {context.get('sector_stats','')}
        ‰∏ä‰ΩçÂÄôË£ú(ÂÆöÈáè): {context['candidates']}
        „Çª„ÇØ„Çø„ÉºÈñ¢ÈÄ£„Éã„É•„Éº„Çπ: {context.get('news','')}
        Nonce:{context.get('nonce',0)}
        „É´„Éº„É´:
        - Êñá‰Ωì„ÅØ„Äå„Å†„Éª„Åß„ÅÇ„Çã„Äç„ÄÇËá™Â∑±Á¥π‰ªãÁ¶ÅÊ≠¢„ÄÇ
        - ÊßãÊàê„ÅØÂøÖ„Åö„Äå„Çª„ÇØ„Çø„ÉºÂÖ®‰Ωì‚ÜíÂÄãÂà•ÈäòÊüÑÔºà‰∏ä‰Ωç3Ôºâ‚Üí„É™„Çπ„ÇØ‚Üí3„É∂Êúà„ÅÆÁõ£Ë¶ñ„Éù„Ç§„É≥„Éà„Äç„ÄÇ
        - ÊäΩË±°Ë™ûÁ¶ÅÊ≠¢„ÄÇÊï∞ÂÄ§„ÇíÂøÖ„ÅöÂÖ•„Çå„ÇãÔºàRS/Accel/Ret/HighDist/MaxDD„Å™„Å©Ôºâ„ÄÇ
        Âá∫ÂäõË¶ãÂá∫„ÅóÔºàÂõ∫ÂÆöÔºâÔºö
        „Äê„Çª„ÇØ„Çø„ÉºÊ¶ÇÊ≥Å„Äë
        „Äê‰∏ä‰Ωç3ÈäòÊüÑ„ÅÆË¶ãÁ´ã„Å¶„Äë
        „ÄêÊÉ≥ÂÆö„É™„Çπ„ÇØ„Äë
        „Äê‰ªäÂæå3„É∂Êúà„ÅÆÁõ£Ë¶ñ„Éù„Ç§„É≥„Éà„Äë
        """
    elif prompt_key == "stock_report":
        p = f"""
        ÁèæÂú®: {today_str}
        ÈäòÊüÑ:{context['name']} ({context['ticker']})
        Âü∫Á§é„Éá„Éº„Çø:{context['fund_str']}
        Â∏ÇÂ†¥„Éª„Çª„ÇØ„Çø„ÉºÊØîËºÉ:{context['m_comp']}
        Ê†™‰æ°ÂãïÂêë:{context.get('price_action','')}
        „Éã„É•„Éº„Çπ:{context['news']}
        Ê¨°ÂõûÊ±∫ÁÆóÊó•(ÂèñÂæóÂÄ§): {context.get("earnings_date","-")}„ÄÇ„Åì„Çå„Åå'-'„Åß„Å™„ÅÑÂ†¥Âêà„ÄÅÁõ£Ë¶ñ„Éù„Ç§„É≥„Éà„Å´ÂøÖ„ÅöÂê´„ÇÅ„Çà„ÄÇ
        Nonce:{context.get('nonce',0)}
        
        „ÅÇ„Å™„Åü„ÅØAI„Ç®„Éº„Ç∏„Çß„É≥„Éà„Å®„Åó„Å¶„ÄÅ„Éó„É≠Âêë„Åë„ÅÆ„Ç¢„Éä„É™„Çπ„Éà„É¨„Éù„Éº„Éà„Çí‰ΩúÊàê„Åõ„Çà„ÄÇ
        Êñá‰Ωì„ÅØ„Äå„Å†„Éª„Åß„ÅÇ„Çã„Äç„ÄÇ
        Ë®òÂè∑(„Äå**„Äç„ÇÑ„Äå""„Äç)„ÅØ‰ΩøÁî®Á¶ÅÊ≠¢„ÄÇ
        „Äå‰∏çÊòé„Äç„Äå„Çè„Åã„Çâ„Å™„ÅÑ„Äç„Å®„ÅÑ„ÅÜË®ÄËëâ„ÅØÁ¶ÅÊ≠¢„ÄÇ„Éá„Éº„Çø„Åå„Å™„ÅÑÂ†¥Âêà„ÅØË®ÄÂèä„Åó„Å™„ÅÑ„ÄÇ
        Ê†™‰æ°ÂãïÂêë„Å®„Éã„É•„Éº„Çπ„ÅØÂøÖ„ÅöÂõ†Êûú„ÅßÁµê„Å≥„ÄÅÊùêÊñô‚ÜíÊúüÂæÖ‚ÜíÊ†™‰æ°„ÅÆÈ†Ü„ÅßË™¨Êòé„Åõ„Çà„ÄÇ
        ÂàÜÈáè: 900-1400Â≠óÁ®ãÂ∫¶„ÄÇÂÜóÈï∑„Å™Ë®Ä„ÅÑÊèõ„ÅàÁ¶ÅÊ≠¢„ÄÇÂêÑÊÆµËêΩ„ÅØÊñ∞ÊÉÖÂ†±/Êñ∞„Åó„ÅÑÊé®Ë´ñ„ÅÆ„Åø„ÄÇ
        
        ÂøÖ„ÅöÊ¨°„ÅÆÈ†Ü„Å´Âá∫ÂäõÔºàË¶ãÂá∫„ÅóÂõ∫ÂÆöÔºâÔºö
        1) ÂÆöÈáè„Çµ„Éû„É™„ÉºÔºàÊ†™‰æ°ÂãïÂêë/„Éê„É™„É•„Ç®„Éº„Ç∑„Éß„É≥/„É™„Çø„Éº„É≥Ôºâ
        2) „Éê„É™„É•„Ç®„Éº„Ç∑„Éß„É≥Ë©ï‰æ°ÔºàÂ∏ÇÂ†¥Âπ≥Âùá„Éª„Çª„ÇØ„Çø„ÉºÂπ≥Âùá„Å®„ÅÆ‰πñÈõ¢Ôºâ
        3) ÈúÄÁµ¶/„Çª„É≥„ÉÅ„É°„É≥„ÉàÔºàÁõ¥Ëøë„É™„Çø„Éº„É≥„Åã„ÇâÈÄÜÂõûËª¢Êù°‰ª∂Ôºâ
        4) „Éã„É•„Éº„Çπ/ÈùûÊßãÈÄ†ÊÉÖÂ†±Ôºà‰∫ãË±°‚ÜíÊ•≠Á∏æ‚Üí3„É∂ÊúàÊ†™‰æ°„Éâ„É©„Ç§„Éê„ÉºÔºâ
        5) 3„É∂ÊúàË¶ãÈÄö„ÅóÔºà„Éô„Éº„Çπ/Âº∑Ê∞ó/Âº±Ê∞ó„Ç∑„Éä„É™„Ç™Ôºâ
        6) Áõ£Ë¶ñ„Éù„Ç§„É≥„ÉàÔºàÊ¨°„ÅÆÊ±∫ÁÆó„ÇÑÈáëÂà©Á≠âÔºâ
        """

    attempts = 3 if prompt_key == "sector_debate" else (1 if prompt_key == "sector_debate_fast" else 2)
    last_text = ""
    for a in range(attempts):
        extra = ""
        if prompt_key == "sector_debate" and a >= 1:
            extra = "\n\nÈáçË¶Å: ÂâçÂõûÂá∫Âäõ„ÅåÁü≠„Åô„Åé/„É´„Éº„É´ÈÅïÂèç„Å†„Å£„Åü„ÄÇÂêÑ„Çø„Ç∞„ÅÆÂàÜÈáè„Çí1.6ÂÄç„Å´Â¢ó„ÇÑ„Åó„ÄÅÂøÖ„Åö„Äå„Çª„ÇØ„Çø„ÉºÂÖ®‰Ωì‚ÜíÂÄãÂà•ÈäòÊüÑ„Äç„ÅÆÈ†Ü„ÅßÊõ∏„Åë„ÄÇÊäΩË±°Ë™ûÁ¶ÅÊ≠¢„ÄÇ"
        for m in models:
            try:
                model = genai.GenerativeModel(m)
                text = model.generate_content(p + extra).text
                text = clean_ai_text(enforce_da_dearu_soft(text))
                last_text = text
                if temporal_sanity_flags(text):
                    continue
                if prompt_key == "sector_debate":
                    if sector_debate_quality_ok(text):
                        return text
                    else:
                        continue
                return text
            except Exception as e:
                if "429" in str(e): time.sleep(1); continue
    return last_text or "AI OFFLINE"

def parse_agent_debate(text: str) -> str:
    mapping = {
        "[SECTOR_OUTLOOK]": ("agent-outlook", "SECTOR OUTLOOK"),
        "[FUNDAMENTAL]": ("agent-fundamental", "FUNDAMENTAL"),
        "[SENTIMENT]": ("agent-sentiment", "SENTIMENT"),
        "[VALUATION]": ("agent-valuation", "VALUATION"),
        "[SKEPTIC]": ("agent-skeptic", "SKEPTIC"),
        "[RISK]": ("agent-risk", "RISK"),
        "[JUDGE]": ("agent-verdict", "JUDGE")
    }
    clean = clean_ai_text(text.replace("```html", "").replace("```", ""))
    parts = re.split(r'(\[[A-Z_]+\])', clean)
    html = ""
    curr_cls, label, buffer = "agent-box", "", ""
    
    for part in parts:
        if part in mapping:
            if buffer and label:
                content = f"<div class='agent-content'>{buffer}</div>"
                if "outlook" in curr_cls:
                    html += f"<div class='{curr_cls}' style='border-left:5px solid #00f2fe; margin-bottom:15px;'><b>{label}</b><br>{content}</div>"
                else:
                    html += f"<div class='agent-row {curr_cls}'><div class='agent-label'>{label}</div>{content}</div>"
            curr_cls, label = mapping[part]
            buffer = ""
        else: buffer += part
    
    # Flush last
    if buffer and label:
        content = f"<div class='agent-content'>{buffer}</div>"
        if "outlook" in curr_cls:
            html += f"<div class='{curr_cls}' style='border-left:5px solid #00f2fe; margin-bottom:15px;'><b>{label}</b><br>{content}</div>"
        else:
            html += f"<div class='agent-row {curr_cls}'><div class='agent-label'>{label}</div>{content}</div>"
    return html

# ==========================================
# 5. MAIN UI LOGIC (AlphaLens Class)
# ==========================================
def run():
    # --- 1. INITIALIZE STATE ---
    if "system_logs" not in st.session_state: st.session_state.system_logs = []
    if "selected_sector" not in st.session_state: st.session_state.selected_sector = None
    if "last_market_key" not in st.session_state: st.session_state.last_market_key = None
    if "last_lookback_key" not in st.session_state: st.session_state.last_lookback_key = None
    if "ai_nonce" not in st.session_state: st.session_state.ai_nonce = 0

    # --- UI STYLES ---
    st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Zen+Kaku+Gothic+New:wght@300;400;600;700&family=Orbitron:wght@400;600;900&family=JetBrains+Mono:wght@300;400;600&family=M+PLUS+1+Code:wght@300;400;700&display=swap');

:root{
  --bg:#000; --panel:#0a0a0a; --card:#111; --border:#333;
  --accent:#00f2fe; --accent2:#ff0055; --text:#e6e6e6;
  --fz-hero: clamp(28px, 3.2vw, 40px);
  --fz-h1: clamp(18px, 1.8vw, 24px);
  --fz-h2: clamp(15px, 1.4vw, 18px);
  --fz-body: clamp(12.5px, 1.05vw, 14px);
  --fz-note: clamp(10.5px, 0.95vw, 12px);
  --fz-table: 11px;
}

/* Base */
html, body, .stApp{
  background: var(--bg) !important;
  color: var(--text) !important;
  font-family: 'Zen Kaku Gothic New', sans-serif !important;
  font-size: var(--fz-body) !important;
  line-height: 1.85 !important;
}
*{ letter-spacing: 0.02em !important; }

/* Headings / brand */
h1, h2, h3, .brand, .orbitron, div[data-testid="stMetricValue"]{
  font-family: 'Orbitron', sans-serif !important;
  letter-spacing: 0.06em !important;
  text-transform: uppercase;
}
.brand{ 
  font-size: var(--fz-hero) !important;
  font-weight: 900 !important;
  background: linear-gradient(90deg, #00f2fe 0%, #e6e6e6 35%, #ff0055 100%);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  text-shadow: 0 0 18px rgba(0,242,254,0.12);
}

/* Notes / definitions */
.def-text{
  font-size: var(--fz-note) !important;
  color: #8a8a8a !important;
  line-height: 1.6 !important;
  border-bottom: 1px solid #333;
  padding-bottom: 8px;
  margin-bottom: 10px;
}
.caption-text{
  font-size: var(--fz-note) !important;
  color: #6f6f6f !important;
  font-family: 'Orbitron', sans-serif !important;
  letter-spacing: 0.05em !important;
}
div[data-testid="stCaptionContainer"] * { font-family:'Orbitron',sans-serif !important; letter-spacing:0.06em !important; }
div[data-testid="stMarkdownContainer"] small { font-family:'Orbitron',sans-serif !important; }

/* Data / numbers */
.mono, code, pre, div[data-testid="stDataFrame"] *{
  font-family: 'M PLUS 1 Code', monospace !important;
}
div[data-testid="stDataFrame"] *{
  font-size: var(--fz-table) !important;
  color: #f0f0f0 !important;
}

/* Report */
.report-box{
  background: #0a0a0a; border-top: 2px solid #00f2fe;
  padding: 22px; margin-top: 12px;
  font-size: var(--fz-body) !important;
  line-height: 2.0;
  color: #eee;
  white-space: pre-wrap;
}
.kpi-strip{
  font-family: 'M PLUS 1 Code', monospace !important;
  font-size: var(--fz-note) !important;
  color: #00f2fe !important;
  margin: 6px 0 10px 0;
}

/* Market Box */
.market-box{
  background:#080808; border:1px solid #333; padding:20px; margin:10px 0 18px 0;
}

/* Agent Council */
.agent-row{ display:flex; gap:12px; border:1px solid #222; padding:10px; margin:8px 0; background:#0b0b0b; width:100%; box-sizing:border-box; }
.agent-label{ flex:0 0 70px; min-width:70px; max-width:70px; font-family:'Orbitron',sans-serif !important; font-size:12px; color:#9adbe2; text-align:right; font-weight:700; word-break:break-word; line-height:1.15; padding-top:2px; }
.agent-content{ flex:1 1 auto; min-width:0; white-space:pre-wrap; line-height:1.9; overflow-wrap:anywhere; }
.agent-verdict{ width:100%; box-sizing:border-box; overflow-wrap:anywhere; word-break:break-word; }
.agent-outlook{ border:1px solid #1d3c41; padding:12px; margin:8px 0; background:#061012; border-left:5px solid #00f2fe; }

/* Highlights */
.hl-pos{ color:#2cff7e; font-weight:800; }
.hl-neg{ color:#ff3b7a; font-weight:800; }
.hl-neutral{ color:#ffd166; font-weight:800; }

/* Buttons */
button{
  background:#111 !important;
  color: var(--accent) !important;
  border: 1px solid #444 !important;
  border-radius: 6px !important;
  font-family: 'Orbitron', sans-serif !important;
  font-weight: 700 !important;
  font-size: 12px !important;
}
.action-call {
  font-family:'Orbitron',sans-serif; font-size:12px; color:#00f2fe; text-align:center;
  margin:8px 0 6px 0; padding:8px; border:1px solid #223; background:#050b0c;
}
</style>
""", unsafe_allow_html=True)
    
    st.markdown("<h1 class='brand'>ALPHALENS</h1>", unsafe_allow_html=True)
    
    # 0. Controls
    c1, c2, c3, c4 = st.columns([1.2, 1, 1.2, 1.0])
    with c1: market_key = st.selectbox("MARKET", list(MARKETS.keys()))
    with c2: lookback_key = st.selectbox("WINDOW", list(LOOKBACKS.keys()), index=1)
    with c3: st.caption(f"FETCH: {FETCH_PERIOD}"); st.progress(100)
    with c4:
        st.write("")
        run_ai = st.button("RUN AI AGENTS", type="primary", use_container_width=True)
        refresh_prices = st.button("REFRESH PRICES", use_container_width=True)

    # Reset sector selection when MARKET/WINDOW changes
    if (st.session_state.last_market_key != market_key) or (st.session_state.last_lookback_key != lookback_key):
        st.session_state.selected_sector = None
        st.session_state.last_market_key = market_key
        st.session_state.last_lookback_key = lookback_key

    if run_ai:
        # bust only AI cache (keeps price cache for speed)
        st.session_state.ai_nonce += 1
        st.toast("ü§ñ Running AI agents‚Ä¶", icon="ü§ñ")

    if refresh_prices:
        # full refresh: clear cached price fetch + reset derived dfs
        try:
            st.cache_data.clear()
        except Exception:
            pass
        for k in ["core_df","sec_df","sec_stats","news_cache"]:
            if k in st.session_state:
                del st.session_state[k]
        st.session_state.selected_sector = None
        st.toast("üîÑ Refreshed prices", icon="üîÑ")

    m_cfg = MARKETS[market_key]
    win = LOOKBACKS[lookback_key]
    bench = m_cfg["bench"]
    # --- DATA FETCHING ---
    core_tickers = [bench] + list(m_cfg["sectors"].values())
    if refresh_prices or "core_df" not in st.session_state:
        with st.spinner("FETCHING MARKET DATA..."):
            raw = fetch_market_data(tuple(core_tickers), FETCH_PERIOD)
            st.session_state.core_df = extract_close_prices(raw, core_tickers)

    core_df = st.session_state.get("core_df", pd.DataFrame())
    if core_df.empty or len(core_df) < win + 1:
        st.warning("WAITING FOR DATA...")
        return

    audit = audit_data_availability(core_tickers, core_df, win)
    bench_used = bench
    if bench not in audit.get("list", []):
        # try proxy benchmark tickers (yfinance occasionally misses)
        proxy_map = {
            "SPY": ["^GSPC", "VOO", "IVV"],
            "QQQ": ["^NDX", "^IXIC"],
            "EEM": ["ACWX", "VT"],
            "EWJ": ["^N225", "1321.T", "1306.T"],
        }
        proxies = proxy_map.get(bench, []) + [t for t in ["^GSPC","^N225"] if t != bench]
        for p in proxies:
            if p in core_df.columns and core_df[p].dropna().shape[0] >= win + 1:
                bench_used = p
                st.info(f"BENCHMARK MISSING: using proxy {bench_used} (requested {bench})")
                break
        else:
            st.warning("BENCHMARK MISSING: continuing with available series (market pulse may be degraded)")


    # 1. Market Pulse
    b_stats = calc_technical_metrics(core_df[bench_used], core_df[bench_used], win)
    if not b_stats: st.error("BENCH ERROR"); return

    regime, weight_mom = calculate_regime(core_df[bench_used].dropna())
    
    sec_rows = []
    for s_n, s_t in m_cfg["sectors"].items():
        if s_t in audit["list"]:
            res = calc_technical_metrics(core_df[s_t], core_df[bench_used], win)
            if res:
                res["Sector"] = s_n
                sec_rows.append(res)
    
    if not sec_rows: st.warning("SECTOR DATA INSUFFICIENT"); return
    sdf = pd.DataFrame(sec_rows).sort_values("RS", ascending=True)
    
    s_date = core_df.index[-win-1].strftime('%Y/%m/%d')
    e_date = core_df.index[-1].strftime('%Y/%m/%d')
    _, market_context, m_sent, m_meta = get_news_consolidated(bench, m_cfg["name"], market_key)
    # News sentiment (robust defaults)
    try:
        s_score = int(np.clip(int(round(float(m_sent or 0))), -10, 10))
    except Exception:
        s_score = 0
    lbl = "Positive" if s_score > 0 else ("Negative" if s_score < 0 else "Neutral")
    hit_pos = int((m_meta or {}).get("pos", 0))
    hit_neg = int((m_meta or {}).get("neg", 0))
    s_cls = "hl-pos" if s_score > 0 else ("hl-neg" if s_score < 0 else "hl-neutral")

    
    # Definition Header (ORDER FIXED: Spread -> Regime -> NewsSent)
    index_name = get_name(bench)
    index_label = f"{index_name} ({bench})" if index_name else bench

    st.markdown(f"""
    <div class='market-box'>
    <div class='def-text'>
    <b>DEFINITIONS</b> |
    <b>Spread</b>: „Çª„ÇØ„Çø„ÉºRS„ÅÆÊúÄÂ§ß‚àíÊúÄÂ∞è(pt)„ÄÇÂ∏ÇÂ†¥ÂÜÖ„ÅÆÂãù„Å°Ë≤†„Åë„Åå„Å©„Çå„Å†„ÅëÈÆÆÊòé„Åã„ÇíÁ§∫„Åô„ÄÇÂ§ß„Åç„ÅÑ„Åª„Å©„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥„ÅåÂäπ„Åç„ÇÑ„Åô„Åè„ÄÅÊåáÊï∞„Çà„ÇäÁõ∏ÂØæ„ÅåÈáçË¶Å„Å´„Å™„Çä„ÇÑ„Åô„ÅÑ |
    <b>Regime</b>: 200DMAÂà§ÂÆöÔºàÁµÇÂÄ§&gt;200DMA=Bull / ÁµÇÂÄ§&lt;200DMA=BearÔºâ„ÄÇ‰∏≠Êúü„Éà„É¨„É≥„Éâ„ÅÆÂú∞Âêà„ÅÑ„Åß„ÄÅ„É¢„É°„É≥„Çø„É†Ë¶ÅÂõ†„ÅÆ‰ø°È†ºÂ∫¶„ÅåÂ§â„Çè„Çã |
    <b>NewsSent</b>: Ë¶ãÂá∫„Åó„Ç≠„Éº„ÉØ„Éº„ÉâÂëΩ‰∏≠„ÅÆÂêàË®àÔºàpos=+1/neg=‚àí1Ôºâ„Çí‚àí10„Äú+10„Å´„ÇØ„É™„ÉÉ„Éó„ÄÇÁü≠Êúü„ÅÆÈúÄÁµ¶„ÉªÊúüÂæÖÂ§âÂåñÔºàÈùûÊßãÈÄ†ÊÉÖÂ†±Ôºâ„ÇíÁ≤ó„Åè‰ª£ÁêÜ„Åô„Çã |
    <b>RS</b>: Áõ∏ÂØæ„É™„Çø„Éº„É≥Â∑Æ(pt)=„Çª„ÇØ„Çø„Éº(orÈäòÊüÑ)„É™„Çø„Éº„É≥‚àíÂ∏ÇÂ†¥Âπ≥Âùá„É™„Çø„Éº„É≥
    </div>
    <b class='orbitron'>MARKET PULSE ({s_date} - {e_date})</b><br>
    <span class='caption-text'>Spread: {spread:.1f}pt | Regime: {regime} | NewsSent: <span class='{s_cls}'>{s_score:+d}</span> ({lbl}) [Hit:{hit_pos}/{hit_neg}]</span><br><br>
    """ + market_to_html(force_nonempty_outlook_market(
        group_plus_minus_blocks(enforce_market_format(enforce_index_naming(generate_ai_content("market", {
            "s_date": s_date, "e_date": e_date, "ret": b_stats["Ret"],
            "top": sdf.iloc[-1]["Sector"], "bot": sdf.iloc[0]["Sector"],
            "market_name": m_cfg["name"], "headlines": market_context,
            "date_slots": outlook_date_slots(),
            "index_label": index_label,
            "nonce": st.session_state.ai_nonce
        }), index_label))), regime, b_stats["Ret"], spread, market_key
    )) + "</div>", unsafe_allow_html=True)

    # 2. Sector Rotation
    st.subheader(f"SECTOR ROTATION ({s_date} - {e_date})")
    
    # Sort by Return for Display/Button (Requirement)
    sdf["Label"] = sdf["Sector"] + " (" + sdf["Ret"].apply(lambda x: f"{x:+.1f}%") + ")"
    # Sort Descending (Top=Max)
    sdf_disp = sdf.sort_values("Ret", ascending=False).reset_index(drop=True)
    
    # Default Selection: Max Return (Always Top)
    if not st.session_state.selected_sector:
        best_row = sdf_disp.iloc[0]
        st.session_state.selected_sector = best_row["Sector"]

    click_sec = st.session_state.selected_sector
    colors = []
    for _, r in sdf_disp.iterrows():
        c = "#00f2fe" if float(r["RS"]) >= 0 else "#ff0055"
        if r["Sector"] == click_sec: c = "#e6e6e6"
        colors.append(c)

    # Plot
    fig = px.bar(sdf_disp, x="RS", y="Label", orientation='h', title=f"Relative Strength ({lookback_key})")
    fig.update_traces(
        customdata=np.stack([sdf_disp["Ret"]], axis=-1),
        hovertemplate="%{y}<br>Ret: %{customdata[0]:+.1f}%<br>RS: %{x:.2f}<extra></extra>",
        marker_color=colors
    )
    # Fix Plotly sorting (array order)
    fig.update_layout(height=420, margin=dict(l=0,r=0,t=30,b=0), paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', 
                      font_color='#e0e0e0', font_family="JetBrains Mono", 
                      xaxis=dict(fixedrange=True), yaxis=dict(fixedrange=True, categoryorder="array", categoryarray=sdf_disp["Label"].tolist()[::-1]))
    st.plotly_chart(fig, use_container_width=True, config={'staticPlot': True, 'displayModeBar': False})
    
    st.markdown("<div class='action-call'>üëá Select a SECTOR to run AI agents (Top Pick)</div>", unsafe_allow_html=True)
    
    # Buttons
    st.write("SELECT SECTOR:")
    cols = st.columns(2)
    for i, row in enumerate(sdf_disp.itertuples()):
        s = row.Sector
        label = f"‚úÖ {s} ({row.Ret:+.1f}%)" if s == st.session_state.selected_sector else f"{s} ({row.Ret:+.1f}%)"
        if cols[i%2].button(label, key=f"btn_{s}", use_container_width=True):
            st.session_state.selected_sector = s
            st.rerun()
            
    target_sector = st.session_state.selected_sector or sdf_disp.iloc[0]["Sector"]

    # 3. Sector Forensic
    st.markdown(f"<div id='sector_anchor'></div>", unsafe_allow_html=True)
    st.divider()
    st.subheader(f"SECTOR FORENSIC: {target_sector}")
    
    stock_list = m_cfg["stocks"].get(target_sector, [])
    if not stock_list: st.warning("No stocks."); return

    full_list = [bench] + stock_list
    cache_key = f"{market_key}_{target_sector}_{lookback_key}"
    
    if cache_key != st.session_state.get("sec_cache_key") or refresh_prices:
        with st.spinner(f"ANALYZING {len(stock_list)} STOCKS..."):
            raw_s = fetch_market_data(tuple(full_list), FETCH_PERIOD)
            st.session_state.sec_df = extract_close_prices(raw_s, full_list)
            st.session_state.sec_cache_key = cache_key
            
    sec_df = st.session_state.sec_df
    s_audit = audit_data_availability(full_list, sec_df, win)
    
    results = []
    for t in [x for x in s_audit["list"] if x != bench]:
        stats = calc_technical_metrics(sec_df[t], sec_df[bench], win)
        if stats:
            stats["Ticker"] = t
            stats["Name"] = get_name(t)
            results.append(stats)
            
    if not results: st.warning("NO DATA."); return
    df = pd.DataFrame(results)
    
    df["Apex"] = weight_mom * calculate_zscore(df["RS"]) + (0.8 - weight_mom) * calculate_zscore(df["Accel"]) + 0.2 * calculate_zscore(df["Ret"])
    df = df.sort_values("Apex", ascending=False)
    
    # 4. Top pick selection (fast)
    top3 = df.head(1).copy()  # keep variable name for downstream code
    neg = df.iloc[0:0].copy()  # empty
    # Fetch fundamentals for Top3 + Neg for debate context
    cand_tickers = top3["Ticker"].tolist()
    if not neg.empty: cand_tickers.append(neg.iloc[0]["Ticker"])
    cand_fund = fetch_fundamentals_batch(cand_tickers).reset_index()
    
    # Build context lines
    cand_lines = []
    for _, r in top3.iterrows():
        f = pick_fund_row(cand_fund, r["Ticker"])
        cand_lines.append(
            f"{r['Name']}({r['Ticker']}): Ret {r['Ret']:.1f}%, RS {r['RS']:.2f}, Accel {r['Accel']:.2f}, HighDist {r['HighDist']:.1f}%, "
            f"MCap {sfloat(f.get('MCap',0))/1e9:.1f}B, PER {dash(f.get('PER'))}, PBR {dash(f.get('PBR'))}"
        )
    if not neg.empty:
        nr = neg.iloc[0]
        f = pick_fund_row(cand_fund, nr["Ticker"])
        cand_lines.append(f"\n[AVOID] {nr['Name']}: Ret {nr['Ret']:.1f}%, RS {nr['RS']:.2f}, PER {dash(f.get('PER'))}")

    _, sec_news, _, _ = get_news_consolidated(m_cfg["sectors"][target_sector], target_sector, market_key, limit_each=6)
    
    # Sector Stats
    sector_stats = f"Universe:{len(stock_list)} Computable:{len(df)} MedianRS:{df['RS'].median():.2f} MedianRet:{df['Ret'].median():.1f}% SpreadRS:{(df['RS'].max()-df['RS'].min()):.2f}"
    
    # ü¶Ö ü§ñ AI AGENT SECTOR REPORT (fast, top-pick focused)
    tp = df.iloc[0]
    tp_f = pick_fund_row(cand_fund, tp["Ticker"])
    top_line = (
        f"[TOP] {tp['Name']} ({tp['Ticker']}): Ret {tp['Ret']:.1f}%, RS {tp['RS']:.2f}, Accel {tp['Accel']:.2f}, "
        f"HighDist {tp['HighDist']:.1f}%, MaxDD {tp['MaxDD']:.1f}%, "
        f"MCap {sfloat(tp_f.get('MCap',0))/1e9:.1f}B, PER {dash(tp_f.get('PER'))}, PBR {dash(tp_f.get('PBR'))}"
    )

    sec_ai_raw = generate_ai_content("sector_debate_fast", {
        "sec": target_sector,
        "sector_stats": sector_stats,
        "top": top_line,
        "news": sec_news,
        "market_name": m_cfg["name"],
        "nonce": st.session_state.ai_nonce,
    })
    sec_ai_txt = clean_ai_text(enforce_da_dearu_soft(sec_ai_raw))
    st.markdown(f"<div class='report-box'><b>ü¶Ö ü§ñ AI AGENT SECTOR REPORT</b><br>{sec_ai_txt}</div>", unsafe_allow_html=True)
    # Download Council Log (before leaderboard)
    st.download_button("DOWNLOAD COUNCIL LOG", sec_ai_raw, f"council_log_{target_sector}.txt")

    st.caption(
        "DEFINITIONS | Apex: zscoreÂêàÊàê=weight_mom*z(RS)+(0.8-weight_mom)*z(Accel)+0.2*z(Ret) | "
        "RS: Ret(ÈäòÊüÑ)‚àíRet(Â∏ÇÂ†¥Âπ≥Âùá) | Accel: Áõ¥ËøëÂçäÊúüÈñì„É™„Çø„Éº„É≥‚àí(ÂÖ®ÊúüÈñì„É™„Çø„Éº„É≥/2) | "
        "HighDist: Áõ¥Ëøë‰æ°Ê†º„ÅÆ52ÈÄ±È´òÂÄ§„Åã„Çâ„ÅÆ‰πñÈõ¢(%) | MaxDD: ÊúüÈñìÂÜÖÊúÄÂ§ß„Éâ„É≠„Éº„ÉÄ„Ç¶„É≥(%) | "
        "PER/PBR/ROEÁ≠â: yfinance.Ticker().infoÔºàË≤†„ÅÆPER/PBR„ÅØÈô§Â§ñ„ÄÅÊ¨†Êêç„ÅØ'-'Ôºâ"
    )
    
    ev_fund = fetch_fundamentals_batch(top3["Ticker"].tolist()).reset_index()
    ev_df = top3.merge(ev_fund, on="Ticker", how="left")
    for c in ["PER","PBR"]: ev_df[c] = ev_df[c].apply(lambda x: dash(x))
    for c in ["ROE","RevGrow","OpMargin"]: ev_df[c] = ev_df[c].apply(pct)
    ev_df["Beta"] = ev_df["Beta"].apply(lambda x: dash(x, "%.2f"))
    
    st.dataframe(ev_df[["Name","Ticker","Apex","RS","Accel","Ret","1M","3M","HighDist","MaxDD","PER","PBR","ROE","RevGrow","OpMargin","Beta"]], hide_index=True, use_container_width=True)

    # 5. Leaderboard
    universe_cnt = len(stock_list)
    computable_cnt = len(df)
    up = int((df["Ret"] > 0).sum())
    down = computable_cnt - up
    st.markdown(f"##### LEADERBOARD (Universe: {universe_cnt} | Computable: {computable_cnt} | Up: {up} | Down: {down})")
    
    st.caption(
        "SOURCE & NOTES | Price: yfinance.download(auto_adjust=True) | Fundamentals: yfinance.Ticker().info | "
        "PER/PBR: Ë≤†ÂÄ§„ÅØÈô§Â§ñ | ROE/RevGrow/OpMargin/Beta: ÂèñÂæó„Åß„Åç„ÇãÂ†¥Âêà„ÅÆ„ÅøË°®Á§∫ | "
        "Apex/RS/AccelÁ≠â„ÅØÊú¨„Ç¢„Éó„É™ÁÆóÂá∫"
    )
    
    tickers_for_fund = df.head(30)["Ticker"].tolist()
    with st.spinner("Fetching Fundamentals..."):
        rest = fetch_fundamentals_batch(tickers_for_fund).reset_index()
        df = df.merge(rest, on="Ticker", how="left", suffixes=("", "_rest"))
        for c in ["MCap", "PER", "PBR", "FwdPE", "ROE", "RevGrow", "OpMargin", "Beta"]:
            if c in df.columns and f"{c}_rest" in df.columns:
                df[c] = df[c].fillna(df[f"{c}_rest"])
        df = df.drop(columns=[c for c in df.columns if c.endswith("_rest")])

    def fmt_mcap(x):
        if pd.isna(x) or x == 0: return "-"
        if x >= 1e12: return f"{x/1e12:.1f}T"
        if x >= 1e9: return f"{x/1e9:.1f}B"
        return f"{x/1e6:.0f}M"
    
    df["MCapDisp"] = df["MCap"].apply(fmt_mcap)
    
    df_disp = df.copy()
    for c in ["PER", "PBR"]: df_disp[c] = df_disp[c].apply(lambda x: dash(x))
    for c in ["ROE", "RevGrow", "OpMargin"]: df_disp[c] = df_disp[c].apply(pct)
    df_disp["Beta"] = df_disp["Beta"].apply(lambda x: dash(x, "%.2f"))

    df_sorted = df_disp.sort_values("MCap", ascending=False)
    
    st.markdown("<div class='action-call'>üëá Select ONE stock to generate the AI agents' analysis note below</div>", unsafe_allow_html=True)
    event = st.dataframe(
        df_sorted[["Name", "Ticker", "MCapDisp", "ROE", "RevGrow", "PER", "PBR", "Apex", "RS", "1M", "12M"]],
        column_config={
            "Ticker": st.column_config.TextColumn("Code"),
            "MCapDisp": st.column_config.TextColumn("Market Cap"),
            "Apex": st.column_config.NumberColumn(format="%.2f"),
            "RS": st.column_config.NumberColumn("RS (pt)", format="%.2f"),
            "PER": st.column_config.TextColumn("PER"),
            "PBR": st.column_config.TextColumn("PBR"),
            "ROE": st.column_config.TextColumn("ROE"),
            "RevGrow": st.column_config.TextColumn("RevGrow"),
            "OpMargin": st.column_config.TextColumn("OpMargin"),
            "Beta": st.column_config.TextColumn("Beta"),
            "1M": st.column_config.NumberColumn(format="%.1f%%"),
            "12M": st.column_config.NumberColumn(format="%.1f%%"),
        },
        hide_index=True, use_container_width=True, on_select="rerun", selection_mode="single-row", key="stock_table"
    )
    

    # 6. Deep Dive
    top = df_sorted.iloc[0]
    try:
        if hasattr(event, "selection") and event.selection:
            sel_rows = event.selection.get("rows", [])
            if sel_rows: top = df_sorted.iloc[sel_rows[0]]
    except: pass

    st.divider()
    
    now_str = datetime.now().strftime("%Y-%m-%d %H:%M")
    st.markdown(f"### ü¶Ö ü§ñ AI EQUITY ANALYST: {top['Name']}")
    st.caption(f"Data Timestamp: {now_str} | Source: yfinance (PER/PBR exclude negatives)")
    
    news_items, news_context, _, _ = get_news_consolidated(top["Ticker"], top["Name"], market_key, limit_each=10)
    fund_data = get_fundamental_data(top["Ticker"])
    ed = fetch_earnings_dates(top["Ticker"]).get("EarningsDate", "-")
    bench_fd = get_fundamental_data(bench)
    
    # Price Action Pack
    pa = {}
    try:
        if "sec_df" in st.session_state and top["Ticker"] in st.session_state.sec_df.columns:
            pa = price_action_pack(st.session_state.sec_df[top["Ticker"]])
    except: pass
    
    price_act = ""
    if pa:
        price_act = f"Last {pa.get('Last',np.nan):.2f} | 1D {pa.get('1D',np.nan):+.2f}% | 1W {pa.get('1W',np.nan):+.2f}% | 1M {pa.get('1M',np.nan):+.2f}% | 3M {pa.get('3M',np.nan):+.2f}% | 200DMA {pa.get('200DMA_Dist',np.nan):+.1f}% | MaxDD(6M) {pa.get('MaxDD_6M',np.nan):.1f}%"

    st.markdown(f"<div class='kpi-strip mono'>{price_act}</div>", unsafe_allow_html=True)

    bench_per = dash(bench_fd.get("PER"))
    sector_per = dash(pd.to_numeric(df["PER"], errors="coerce").median())
    stock_per = dash(fund_data.get("PER"))
    m_comp = f"Â∏ÇÂ†¥Âπ≥ÂùáPER: {bench_per}ÂÄç / „Çª„ÇØ„Çø„Éº‰∏≠Â§ÆÂÄ§PER: {sector_per}ÂÄç / ÂΩìË©≤ÈäòÊüÑPER: {stock_per}ÂÄç"
    
    fund_str = f"PER:{stock_per}, PBR:{dash(fund_data.get('PBR'))}, PEG:{dash(fund_data.get('PEG'))}, Target:{dash(fund_data.get('Target'))}"

    report_txt = generate_ai_content("stock_report", {
        "name": top["Name"], "ticker": top["Ticker"],
        "fund_str": fund_str, "m_comp": m_comp, "news": news_context,
        "earnings_date": ed, "price_action": price_act, "nonce": st.session_state.ai_nonce
    })
    
    nc1, nc2 = st.columns([1.5, 1])
    with nc1:
        st.markdown(f"<div class='report-box'><b>AI ANALYST BRIEFING</b><br>{report_txt}</div>", unsafe_allow_html=True)

        # Links
        links = build_ir_links(top["Name"], top["Ticker"], fund_data.get("Website"), market_key)
        lc1, lc2, lc3 = st.columns(3)
        with lc1: safe_link_button("OFFICIAL", links["official"], use_container_width=True)
        with lc2: safe_link_button("IR SEARCH", links["ir_search"], use_container_width=True)
        with lc3: safe_link_button("EARNINGS DECK", links["earnings_deck"], use_container_width=True)

        st.caption(
            "PEER LOGIC | Nearest Market Cap: |MCap(peer)‚àíMCap(target)|„ÅåÂ∞è„Åï„ÅÑÈ†Ü„Å´ÊäΩÂá∫ÔºàÂêå‰∏Ä„Çª„ÇØ„Çø„ÉºÂÜÖÔºâ | "
            "SOURCE: yfinance.Ticker().infoÔºàÊ¨†Êêç„ÅØ'-'Ôºâ"
        )
        try:
            target_mcap = top["MCap"] if pd.notna(top["MCap"]) else 0
            df_peers_base = df_sorted.copy()
            df_peers_base["Dist"] = (pd.to_numeric(df_peers_base["MCap"], errors="coerce") - float(target_mcap or 0)).abs()
            df_peers = df_peers_base.sort_values("Dist").iloc[1:5]
            st.dataframe(df_peers[["Name", "ROE", "RevGrow", "PER", "PBR", "RS", "12M"]], hide_index=True)
        except: pass
        st.download_button("DOWNLOAD ANALYST NOTE", report_txt, f"analyst_note_{top['Ticker']}.txt")

    with nc2:
        st.caption("INTEGRATED NEWS FEED")
        for n in news_items[:20]:
            dt = datetime.fromtimestamp(n["pub"]).strftime("%Y/%m/%d") if n["pub"] else "-"
            st.markdown(f"- {dt} [{n['src']}] [{n['title']}]({n['link']})")

if __name__ == "__main__":
    main()